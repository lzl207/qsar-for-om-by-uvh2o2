{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a7233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#代码示例，仅做参考"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66b5caf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0.0 + 0.6 * x1 + 1.2000000000000002 * x2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#X = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
    "\n",
    "# 创建线性回归模型\n",
    "model = LinearRegression()\n",
    "\n",
    "# 拟合模型\n",
    "model.fit(X, y)\n",
    "\n",
    "# 提取截距和斜率\n",
    "intercept = model.intercept_\n",
    "slopes = model.coef_\n",
    "\n",
    "# 构建并输出线性回归表达式\n",
    "expression = f\"y = {intercept}\"\n",
    "for i, slope in enumerate(slopes):\n",
    "    expression += f\" + {slope} * x{i + 1}\"\n",
    "\n",
    "print(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b989d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    37.65          5030.08       27          0.71918         0.649023     13.58m\n",
      "   1    11.50           12.762       19         0.104901         0.152864      9.91m\n",
      "   2    13.32          2.92634       17        0.0880679         0.109142     10.85m\n",
      "   3    15.96          3.34298       17        0.0813294          0.16325     10.08m\n",
      "   4    16.71          2.06019       17        0.0832177         0.152793     11.60m\n",
      "   5    15.11          2.00552       13        0.0788785         0.191846     11.61m\n",
      "   6    13.88          1.73466       13        0.0806772         0.175658     10.97m\n",
      "   7    12.54          1144.59       13        0.0804315         0.177869     10.57m\n",
      "   8    11.44          1.71357       13        0.0798434         0.183161      8.86m\n",
      "   9    11.16          3.78348       11        0.0840752         0.190855      9.12m\n",
      "  10    11.04          1.89552       11        0.0845742         0.186363      8.98m\n",
      "  11    11.07          2.36526       11        0.0842435         0.189339      9.17m\n",
      "  12    11.07          4.09652       11        0.0816912          0.21231      9.30m\n",
      "  13    11.15          1.54762       11        0.0825557         0.204529      9.55m\n",
      "  14    11.12           36.042       11        0.0786684         0.197622      8.69m\n",
      "  15    11.10          3.43702       11        0.0801228         0.184533      8.10m\n",
      "  16    11.18          2.53484       11        0.0800039         0.185602      8.47m\n",
      "  17    11.06          1.49189       11        0.0810766         0.175948      8.14m\n",
      "  18    11.08          1.97563       11        0.0803225         0.182735      8.10m\n",
      "  19    11.11          2.45326       11        0.0797505         0.187883      7.86m\n",
      "  20    11.08          1.44592       11        0.0806571         0.179724      8.08m\n",
      "  21    11.15          1.70901       11        0.0798343         0.187129      7.74m\n",
      "  22    11.19          1.34289       11        0.0795293         0.189874      7.84m\n",
      "  23    11.05          1.61733       11        0.0790473         0.194211      7.80m\n",
      "  24    11.10          4.08601       11        0.0792503         0.192385      7.47m\n",
      "  25    10.99          2.78147       11        0.0810568         0.176126      7.52m\n",
      "  26    11.12           24.854       11        0.0800404         0.185274      7.34m\n",
      "  27    11.05          4.04742       11        0.0807675          0.17873      7.27m\n",
      "  28    11.07          16.8445       11        0.0798985         0.186551      7.20m\n",
      "  29    11.17          100.601       11        0.0798297          0.18717      7.24m\n",
      "  30    11.12          1.58932       11        0.0805385         0.180791      6.73m\n",
      "  31    11.05          1.46794       11        0.0799673         0.185932      7.06m\n",
      "  32    11.05          4.33271       11        0.0804883         0.181243      7.83m\n",
      "  33    11.12           1.6512       11        0.0799656         0.185947      6.52m\n",
      "  34    11.07          1.54388       11        0.0794178         0.190878      6.78m\n",
      "  35    11.06          1.85945       11        0.0788112         0.196337      6.38m\n",
      "  36    11.01          1.59236       11        0.0791539         0.193252      6.43m\n",
      "  37    11.14          54.8602       11        0.0802852          0.18307      6.16m\n",
      "  38    11.09          1.54939       11        0.0801785         0.184031      6.22m\n",
      "  39    11.11          3.67504       11        0.0799115         0.186434      6.02m\n",
      "  40    11.14          1.73927       11        0.0803431         0.182549      6.17m\n",
      "  41    10.99          2.48699       11        0.0799915         0.185714      5.80m\n",
      "  42    11.08          1.33789       11        0.0803147         0.182805      5.81m\n",
      "  43    11.07          1.94254       11        0.0775145         0.208006      5.77m\n",
      "  44    11.11          1.76145       11        0.0788662         0.195841      5.54m\n",
      "  45    11.02          1.85411       11         0.080018         0.185476      5.49m\n",
      "  46    11.05          1.58869       11        0.0798725         0.186785      5.33m\n",
      "  47    11.16          2.02817       11        0.0803887         0.182139      5.49m\n",
      "  48    11.08          1.81248       11        0.0804813         0.181306      5.08m\n",
      "  49    11.02          2.52929       11        0.0788188         0.196268      5.29m\n",
      "  50    11.12          15.4921       11         0.077307         0.209874      4.92m\n",
      "  51    11.09          2.70542       11        0.0800722         0.184988      5.04m\n",
      "  52    11.03          1.55843       11         0.078193         0.201901      4.84m\n",
      "  53    11.11           4.9736       11        0.0789089         0.195458      5.58m\n",
      "  54    11.02           1.7489       11        0.0789924         0.194706      4.61m\n",
      "  55    11.13          1.53776       11        0.0804058         0.181985      4.54m\n",
      "  56    11.08          6.54325       11         0.080228         0.183585      4.47m\n",
      "  57    11.10          3.32315       11        0.0807337         0.179034      4.42m\n",
      "  58    11.11           5.8135       11        0.0805001         0.181136      4.28m\n",
      "  59    11.05          2.71433       11        0.0812134         0.174716      4.19m\n",
      "  60    11.09          23.2275       11        0.0804082         0.181963      4.01m\n",
      "  61    11.20           48.035       11        0.0780629         0.203072      4.12m\n",
      "  62    11.06          2.98081       11        0.0775969         0.207265      3.69m\n",
      "  63    11.17          20581.4       11        0.0803835         0.182186      3.73m\n",
      "  64    11.19          1.82166       11        0.0799524         0.186066      3.59m\n",
      "  65    11.12          1.42252       11        0.0812444         0.174438      3.62m\n",
      "  66    11.05          1.89712       11        0.0802302         0.183566      3.58m\n",
      "  67    11.13          2.31929       11         0.079653          0.18876      3.27m\n",
      "  68    11.06          2.28467       11        0.0788374         0.196101      3.35m\n",
      "  69    11.05           61.796       11        0.0795342          0.18983      3.01m\n",
      "  70    11.20          2.41112       11         0.078028         0.203385      3.13m\n",
      "  71    11.18          2.00502       11        0.0798722         0.186787      2.82m\n",
      "  72    11.14          1.87518       11        0.0798332         0.187139      2.93m\n",
      "  73    11.01          3.69864       11        0.0788194         0.196263      2.72m\n",
      "  74    11.10          1.36268       11         0.079189         0.192937      2.54m\n",
      "  75    11.14           4.4843       11         0.078977         0.194844      2.59m\n",
      "  76    11.18          2.35566       11        0.0794777         0.190338      2.33m\n",
      "  77    11.04          293.095       11        0.0802519          0.18337      2.37m\n",
      "  78    11.03          2.00759       11        0.0794009          0.19103      2.12m\n",
      "  79    11.10          1.35876       11         0.078818         0.196276      2.16m\n",
      "  80    11.10          56.4869       11        0.0792826         0.192094      2.36m\n",
      "  81    11.01           2.5653       11        0.0806691         0.179615      2.33m\n",
      "  82    11.07           1.9803       11         0.080256         0.183334      1.92m\n",
      "  83    10.99          1.53529       11        0.0793611         0.191388      1.62m\n",
      "  84    11.07          1.51881       11        0.0803063          0.18288      2.01m\n",
      "  85    11.11          1.86285       11        0.0798637         0.186864      2.29m\n",
      "  86    11.04           252.42       11        0.0803032         0.182909      2.14m\n",
      "  87    11.12          1.61679       11        0.0799787         0.185829      1.82m\n",
      "  88    11.15          1.54018       11        0.0810515         0.176174      1.58m\n",
      "  89    10.99          1.54891       11        0.0795825         0.189395      1.29m\n",
      "  90    11.06          32.1996       11        0.0798092         0.187354      1.06m\n",
      "  91    11.13          12.3665       11        0.0799425         0.186155     56.29s\n",
      "  92    11.12          1.57263       11        0.0792372         0.192502     53.87s\n",
      "  93    11.20          2.84042       11        0.0788456         0.196027     42.36s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  94    11.10          1.68103       11        0.0795967         0.189267     34.77s\n",
      "  95    11.04          1.53722       11         0.080731         0.179058     27.57s\n",
      "  96    11.09          7.10853       11        0.0799688         0.185918     20.34s\n",
      "  97    11.10          2.09699       11        0.0793461         0.191522     14.34s\n",
      "  98    11.06          11.1611       11        0.0801751         0.184062      6.86s\n",
      "  99    11.04          8.38252       11        0.0800871         0.184854      0.00s\n",
      "拟合得到的表达式: add(add(add(X0, X1), add(X0, X1)), add(0.996, X1))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 创建 SymbolicRegressor 模型\n",
    "est_gp = SymbolicRegressor(population_size=10000,\n",
    "                           generations=100, stopping_criteria=0.01,\n",
    "                           p_crossover=0.7, p_subtree_mutation=0.1,\n",
    "                           p_hoist_mutation=0.05, p_point_mutation=0.1,\n",
    "                           max_samples=0.9, verbose=1,\n",
    "                           parsimony_coefficient=0.005, random_state=42)\n",
    "\n",
    "\n",
    "# 拟合模型\n",
    "est_gp.fit(X_train, y_train)\n",
    "\n",
    "# 输出拟合得到的表达式\n",
    "best_expression = est_gp._program\n",
    "print(f\"拟合得到的表达式: {best_expression}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5b79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(X, y)\n",
    "# 获取特征重要性\n",
    "importance = model.feature_importances_\n",
    "\n",
    "\n",
    "# 将特征重要性与特征名对应起来\n",
    "feature_importance = pd.DataFrame({'feature_name': X.columns, 'importance': importance})\n",
    "\n",
    "# 按照重要性排序特征\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "\n",
    "# 打印特征重要性排名\n",
    "print(feature_importance[0:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac40f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 训练模型\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X, y)\n",
    "\n",
    "# 计算 Shap 值\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "# 可视化 Shap 值\n",
    "shap.summary_plot(shap_values, X, max_display=10,plot_size=(4,3.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
